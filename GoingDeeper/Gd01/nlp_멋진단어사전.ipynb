{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e6b7cb",
   "metadata": {},
   "source": [
    "# [NLP] 멋진 단어사전 만들기\n",
    "## Step 1. SentencePiece 설치하기\n",
    "## Step 2. SentencePiece 모델 학습\n",
    "## Step 3. Tokenizer 함수 작성\n",
    "## Step 4. 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff3c72",
   "metadata": {},
   "source": [
    "## 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ef4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c01d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70517ab3",
   "metadata": {},
   "source": [
    "## Step 1. SentencePiece 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369ed955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9e5e5",
   "metadata": {},
   "source": [
    "## Step 2. SentencePiece 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a1d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 네이버 영화리뷰 데이터를 불러옵니다.\n",
    "train_data = pd.read_csv('ratings_train.txt', sep='\\t')\n",
    "test_data = pd.read_csv('ratings_test.txt', sep='\\t')\n",
    "\n",
    "# 결측값 제거\n",
    "train_data = train_data.dropna(how='any')\n",
    "test_data = test_data.dropna(how='any')\n",
    "\n",
    "# 리뷰 문장만 추출\n",
    "corpus = train_data['document'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a67cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                           document  label\n",
      "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "...          ...                                                ...    ...\n",
      "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
      "149996   8549745                                      평점이 너무 낮아서...      1\n",
      "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
      "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
      "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
      "\n",
      "[149995 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf09dfb",
   "metadata": {},
   "source": [
    "### 데이터의 문장 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b8424c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 149995\n",
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 146\n",
      "문장의 평균 길이: 70\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Data Size:\", len(corpus))\n",
    "\n",
    "for sen in corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: \n",
    "        min_len = length\n",
    "    if max_len < length: \n",
    "        max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(corpus))\n",
    "\n",
    "# 문장 길이 분포 시각화\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in corpus:\n",
    "    sentence_length[len(sen) - 1] += 1\n",
    "\n",
    "plt.bar(range(1, max_len + 1), sentence_length, width=1.0)\n",
    "plt.title(\"sentence length distribution\")\n",
    "plt.xlabel(\"sentence length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecbd9eb",
   "metadata": {},
   "source": [
    "### 1. 길이가 1인 문장 파악해보기\n",
    "### 2. 유의미한 데이터 구간 파악해보기\n",
    "### 3. 10부터 50까지 잘라보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911923c8",
   "metadata": {},
   "source": [
    "1. 길이가 1인 문장 파악해보기\n",
    "- 역시 의미가 없거나 이모티콘이기에 노이즈로 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299d38bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아\n",
      "잼\n",
      "1\n",
      "4\n",
      "4\n",
      "굿\n",
      "짱\n",
      "휴\n",
      ".\n",
      "1\n",
      "굿\n",
      "음\n",
      "?\n",
      "?\n",
      "ㅎ\n",
      "굿\n",
      "ㅋ\n",
      "굿\n",
      "즐\n",
      "♥\n",
      "굳\n",
      "ㅋ\n",
      "네\n",
      "ㅎ\n",
      "ㅋ\n",
      "굿\n",
      "ㅇ\n",
      "k\n",
      ".\n",
      "굿\n",
      "굿\n",
      "굳\n",
      "ㅠ\n",
      "?\n",
      "1\n",
      "ㅋ\n",
      "굿\n",
      "쒯\n",
      "굿\n",
      "굿\n",
      "굳\n",
      "♬\n",
      "굿\n",
      "토\n",
      "ㅋ\n",
      "ㅋ\n",
      "굿\n",
      "ㅋ\n",
      "굿\n",
      "O\n",
      "똥\n",
      "ㅎ\n",
      ".\n",
      "굿\n",
      "ㅎ\n",
      "짱\n",
      "굳\n",
      "굿\n",
      "굿\n",
      "짱\n",
      "?\n",
      "z\n",
      "굿\n",
      "짱\n",
      "음\n",
      "굳\n",
      "ㅇ\n",
      "헐\n",
      "굳\n",
      "굳\n",
      "굿\n",
      "굿\n",
      "굿\n",
      "삼\n",
      "꽝\n",
      "굿\n",
      "굿\n",
      "굿\n",
      "굿\n",
      "ㅎ\n",
      "굳\n",
      "굿\n",
      "4\n",
      "!\n",
      "?\n",
      "ㅎ\n",
      "1\n",
      "굳\n",
      ".\n",
      "ㅎ\n",
      "풉\n",
      "아\n",
      "굿\n",
      "똥\n",
      "ㅅ\n",
      "왜\n",
      "ㄴ\n",
      "굳\n",
      "쉣\n",
      "봐\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "def check_sentence_with_length(corpus, length):\n",
    "    count = 0\n",
    "    \n",
    "    for sen in corpus:\n",
    "        if len(sen) == length:\n",
    "            print(sen)\n",
    "            count += 1\n",
    "            if count > 100: return\n",
    "\n",
    "check_sentence_with_length(corpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49552b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Index: 12\n",
      "Outlier Index: 13\n",
      "Outlier Index: 14\n",
      "Outlier Index: 15\n"
     ]
    }
   ],
   "source": [
    "for idx, _sum in enumerate(sentence_length):\n",
    "    # 문장의 수가 4000을 초과하는 문장 길이를 추출합니다.\n",
    "    if _sum > 4000:\n",
    "        print(\"Outlier Index:\", idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0572f523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아햏햏 아햏햏 아햏햏.\n",
      "단연 최고라고 할수있지\n",
      "용가리 진짜짱짱맨이다ㅋ\n",
      "나름 괜찮은 작품입니다\n",
      "정말 실망 스러웟음..\n",
      "무술인이 왜 총을드나?\n",
      "너무너무 훈훈하네요^^\n",
      "현실은 꿈, 꿈은 현실\n",
      "답없네, 뭐하는건지..\n",
      "엔딩이 넘 슬퍼요 :(\n",
      "감동감동감동의 도가니탕\n",
      "정말 최고의 영화...\n",
      "진짜 짜증나는 영화..\n",
      "상당히 재밌게 봤습니다\n",
      "영상미가 역시 최고네요\n",
      "감독ㅡㅡ다신영화찍지마라\n",
      "이런 영화가 참 좋다.\n",
      "정말 광해와 비슷한가?\n",
      "더빙이 똥이야 ....\n",
      "잠을 청할 수 있었다.\n",
      "나오코 진짜 집중안된다\n",
      "샬라샬라 나오다즁자쥬아\n",
      "지루하다.. 지루해..\n",
      "사과해요, 나한테!!!\n",
      "소재가 굉장히 신선했다\n",
      "솔직히 산만 하기만했다\n",
      "OST가 좋은 영화!!\n",
      "기적은 이미 일어났다.\n",
      "답을 알려고 하지마라.\n",
      "평점은 믿지마셈 재밌음\n",
      "재미 드럽게 없다ㅋㅋㅋ\n",
      "자식을 그렇게 때리냐?\n",
      "감동적이였고 좋았습니다\n",
      "울컥하는 사회현실 ㅠㅠ\n",
      "등장인물들 모두 짱~~\n",
      "막장드라마가 따로없구만\n",
      "말 필요엄따~~~~진정\n",
      "꿀잼 영화 추억이다ㅜㅜ\n",
      "맞추자 ㅍ?차븟ㅇㄱ디시\n",
      "순수하고 아름답다 :)\n",
      "변태적 성욕 자기합리화\n",
      "흠....나름 갠찬네요\n",
      "걍 10점 이라길래..\n",
      "실력이 필요없는 상황.\n",
      "감동과 웃음을 한방에!\n",
      "이거야 이거 ㅋㅋㅋㅋㅋ\n",
      "성동일딸 시르다...참\n",
      "평점알바들을 위해 1점\n",
      "볼만합니다 강추!!ㅋㅋ\n",
      "이거말고 겨울왕국을봐라\n",
      "좋은니다..^.,^~~\n",
      "2.3 (10자 제한)\n",
      "아무도 안달았네..ㄷㄷ\n",
      "엉성하고 어설프고...\n",
      "그냥 보통으로 보았다.\n",
      "너무 슬픈영화..울음ㅠ\n",
      "으리는 개뿔 ㅡ,.ㅡ^\n",
      "처음 본 에로 영화..\n",
      "평점 조정을 위해...\n",
      "그냥 쓰레기지 쓰레기~\n",
      "쵝오네요 최고 대박영화\n",
      "유전무죄 무전유죄!!!\n",
      "의외의 꿀잼 이였습니다\n",
      "어차피 우승은 송민호~\n",
      "이 때부터가 쓰레기였지\n",
      "완전 찝찝한 영화...\n",
      "이상형이 바뀌었습니다.\n",
      "역시 산드라블록 누님!\n",
      "결국 엄마가 죽인거네?\n",
      "그냥 그저 별시리...\n",
      "추천할만한 경제영화였음\n",
      "제대로 알아야할 역사임\n",
      "일본판이더재미있음...\n",
      "역시 재미있군요 >ㅁ<\n",
      "아깝다 나의 1점도ㅡㅡ\n",
      "마음이 따뜻해지는 영화\n",
      "죽었다. 참 재미없다.\n",
      "최고, 정말 최고다,,\n",
      "공유 보려고 보는 영화\n",
      "재밌습니다. 꿀잼 ㅋㅋ\n",
      "조정래와 임권택의 만남\n",
      "간만에쓰레기영화보네요ㅋ\n",
      "너무재밌게 봤어요...\n",
      "의미가 뭘까...///\n",
      "이건 범죄 수준이다..\n",
      "박평식-뚜껑열린다ㅋㅋㅋ\n",
      "볼수록 재밌는 마의!!\n",
      "짱~ 오늘 티비로 봤음\n",
      "말없이 눈물이 뚝뚝..\n",
      "이렇게 유쾌할수가ㅋㅋㅋ\n",
      "완죤 지루하고 재미없네\n",
      "보다가 나가는 영화??\n",
      "에휴..........\n",
      "재미도 없구만...에효\n",
      "옥보단 3D보다 쓰레기\n",
      "반전이 꽤나 신선했다.\n",
      "허접허접허접허접허접허접\n",
      "독특한 유머. 재밌다.\n",
      "좀 식상한 스릴러영화~\n",
      "하하하핳하햐하핯하핳하하\n",
      "이영화 어디서 보나요?\n"
     ]
    }
   ],
   "source": [
    "check_sentence_with_length(corpus, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55127c5",
   "metadata": {},
   "source": [
    "### 중복 제거\n",
    "- 149995 -> 146182 로 3800여개 제거\n",
    "- 평균길이 70 -> 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f8cc926",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 146182\n",
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 146\n",
      "문장의 평균 길이: 35\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 보이는건 중복이 없지만 그래도 중복 제거\n",
    "# 중복된 문장을 제거\n",
    "clean_corpus = list(set(corpus))\n",
    "print(\"Data Size:\", len(clean_corpus))\n",
    "\n",
    "# 문장의 최단, 최장, 평균 길이를 계산하는 코드\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "for sen in clean_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: \n",
    "        min_len = length\n",
    "    if max_len < length: \n",
    "        max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(clean_corpus))\n",
    "\n",
    "# 문장 길이 분포 시각화\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in clean_corpus:\n",
    "    sentence_length[len(sen) - 1] += 1\n",
    "\n",
    "plt.bar(range(1, max_len + 1), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.xlabel(\"Sentence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d798b",
   "metadata": {},
   "source": [
    "### 데이터 길이 제거\n",
    "- 분포를 보면 10~50구간 사이 데이터가 가장 많이 분포되어 있는 것을 확인\n",
    "- 데이터 양이 크기 때문에 과감하게 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50397c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 50\n",
    "min_len = 10\n",
    "\n",
    "# 길이 조건에 맞는 문장만 선택합니다.\n",
    "filtered_corpus = [s for s in clean_corpus if (len(s) < max_len) & (len(s) >= min_len)]\n",
    "\n",
    "# 분포도를 다시 그려봅니다.\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in filtered_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b89e3c",
   "metadata": {},
   "source": [
    "### SentencePiece 학습\n",
    "- SentencePieceProcessor 를 사용하여 모델 학습. Subword 기반 토크나이저 구축\n",
    "- model_type은 `BPE(Byte Pair Encoding)` 과 `Unigram` 방식을 선택.\n",
    "- `BPE(Byte Pair Encoding)` 는 자주 발생하는 바이트 쌍을 병합해 서브워드를 만듬(정교함, 데이터 길이를 길게 만듬)\n",
    "- `Unigram` 는 서브워드를 데이터에서 자동으로 확률적 선택하여 어휘를 구성(일반적, 문맥파악)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "060a7d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: filtered_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: filtered_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 109351 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=2952128\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1743\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 109351 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 187824 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 109351\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 213974\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 213974 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=98912 obj=15.4019 num_tokens=504694 num_tokens/piece=5.10245\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=91890 obj=14.365 num_tokens=507817 num_tokens/piece=5.52636\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=68885 obj=14.493 num_tokens=530348 num_tokens/piece=7.69903\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=68780 obj=14.4295 num_tokens=531268 num_tokens/piece=7.72416\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=51582 obj=14.7191 num_tokens=560044 num_tokens/piece=10.8574\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=51575 obj=14.6458 num_tokens=560177 num_tokens/piece=10.8614\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=38681 obj=14.9644 num_tokens=588354 num_tokens/piece=15.2104\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=38681 obj=14.8915 num_tokens=588408 num_tokens/piece=15.2118\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=29010 obj=15.2436 num_tokens=618502 num_tokens/piece=21.3203\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=29010 obj=15.1655 num_tokens=618502 num_tokens/piece=21.3203\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21757 obj=15.5509 num_tokens=649482 num_tokens/piece=29.8516\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21757 obj=15.4743 num_tokens=649488 num_tokens/piece=29.8519\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=16317 obj=15.8889 num_tokens=680752 num_tokens/piece=41.7204\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=16317 obj=15.8065 num_tokens=680841 num_tokens/piece=41.7259\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=12237 obj=16.2614 num_tokens=714312 num_tokens/piece=58.3731\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=12237 obj=16.1683 num_tokens=714518 num_tokens/piece=58.39\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=9177 obj=16.6656 num_tokens=750332 num_tokens/piece=81.7622\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=9177 obj=16.5603 num_tokens=750606 num_tokens/piece=81.7921\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=16.6325 num_tokens=756516 num_tokens/piece=85.9677\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=16.6156 num_tokens=756514 num_tokens/piece=85.9675\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# corpus.txt 파일에 필터링된 문장들을 저장합니다.\n",
    "with open('filtered_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in filtered_corpus:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "# SentencePiece 모델 학습\n",
    "spm.SentencePieceTrainer.train(input='filtered_corpus.txt', model_prefix='korean_spm', vocab_size=8000, model_type='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37fa90b",
   "metadata": {},
   "source": [
    "## Step3. SentencePiece Tokenizer 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af6f37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "\n",
    "# 학습된 SentencePiece 모델 로드\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('korean_spm.model')\n",
    "\n",
    "def sp_tokenize(corpus):\n",
    "    # 문장들을 토큰화하여 ID 시퀀스로 변환\n",
    "    tensor = [sp.EncodeAsIds(sentence) for sentence in corpus]\n",
    "    \n",
    "    # vocab 파일에서 단어 인덱스를 로드하여 word_index와 index_word 사전 생성\n",
    "    with open('korean_spm.vocab', 'r', encoding='utf-8') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {line.split('\\t')[0]: idx for idx, line in enumerate(vocab)}\n",
    "    index_word = {idx: line.split('\\t')[0] for idx, line in enumerate(vocab)}\n",
    "\n",
    "    # 패딩 적용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd655b",
   "metadata": {},
   "source": [
    "## Step 4. 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "873318b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 문장과 레이블 추출\n",
    "train_corpus = train_data['document'].tolist()\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "test_corpus = test_data['document'].tolist()\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# SentencePiece를 사용한 토큰화\n",
    "train_tensor, word_index, index_word = sp_tokenize(train_corpus)\n",
    "test_tensor, _, _ = sp_tokenize(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0d61c",
   "metadata": {},
   "source": [
    "### 모델 설계\n",
    "- LSTM 기반 감성 분석 모델을 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64b6784b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1024000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,155,713\n",
      "Trainable params: 1,155,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D\n",
    "\n",
    "vocab_size = 8000  # SentencePiece에서 설정한 vocab_size와 동일\n",
    "\n",
    "# 감성 분석을 위한 LSTM 모델\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03634369",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d64ec5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2344/2344 [==============================] - 47s 9ms/step - loss: 0.3663 - accuracy: 0.8339 - val_loss: 0.3235 - val_accuracy: 0.8575\n",
      "Epoch 2/5\n",
      "2344/2344 [==============================] - 21s 9ms/step - loss: 0.2839 - accuracy: 0.8779 - val_loss: 0.3127 - val_accuracy: 0.8638\n",
      "Epoch 3/5\n",
      "2344/2344 [==============================] - 21s 9ms/step - loss: 0.2480 - accuracy: 0.8951 - val_loss: 0.3211 - val_accuracy: 0.8633\n",
      "Epoch 4/5\n",
      "2344/2344 [==============================] - 21s 9ms/step - loss: 0.2127 - accuracy: 0.9123 - val_loss: 0.3417 - val_accuracy: 0.8613\n",
      "Epoch 5/5\n",
      "2344/2344 [==============================] - 21s 9ms/step - loss: 0.1739 - accuracy: 0.9295 - val_loss: 0.3664 - val_accuracy: 0.8555\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_tensor, train_labels, epochs=5, batch_size=64, validation_data=(test_tensor, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbce97",
   "metadata": {},
   "source": [
    "### 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75534d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3664 - accuracy: 0.8555\n",
      "Test Accuracy: 0.8555113077163696\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(test_tensor, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 (f1 score, precision, recall)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# 예측값 얻기 (이진 분류이므로 0.5 기준으로 예측)\n",
    "predictions = (model.predict(test_tensor) > 0.5).astype(\"int32\")\n",
    "\n",
    "# F1 Score 계산\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# Precision과 Recall 계산\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc252a4",
   "metadata": {},
   "source": [
    "### SentencePiece 학습(함수/모듈화)\n",
    "- SentencePiece 모델을 학습하는 함수\n",
    "    - 텍스트 파일로 모델 저장\n",
    "    - SentencePiece 모델 학습\n",
    "- SentencePiece Tokenizer 함수\n",
    "- 전체 작업 실행\n",
    "    - 네이버 영화리뷰 데이터 불러오기\n",
    "    \n",
    "    \n",
    "> 실험: vocab size, model type 변경\n",
    "- vocab size: 8000 -> 12000(과적합 위험이 있지만, 더 많은 정보를 학습하여 문맥 정보를 더 잘 반영)\n",
    "- model type: unigram -> BPE(vocab 사이즈를 늘렸다면 등장 빈도가 높은 패턴을 효과적으로 인코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83a290a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 파일: korean_spm.model, korean_spm.vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: filtered_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: BPE\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: filtered_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 149995 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5430559\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1712\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 149995 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 149995\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 357580\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=73192 min_freq=87\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10912 size=20 all=110810 active=10498 piece=▁어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8938 size=40 all=115274 active=14962 piece=▁정말\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6288 size=60 all=118867 active=18555 piece=▁생\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5282 size=80 all=122839 active=22527 piece=드라마\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4218 size=100 all=126198 active=25886 piece=하게\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4185 min_freq=73\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3617 size=120 all=129574 active=9244 piece=~~\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3206 size=140 all=132083 active=11753 piece=▁좀\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2840 size=160 all=134394 active=14064 piece=^^\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2644 size=180 all=137080 active=16750 piece=▁처\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2388 size=200 all=139333 active=19003 piece=!!!\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2382 min_freq=65\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2218 size=220 all=141749 active=9343 piece=▁도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2050 size=240 all=144893 active=12487 piece=▁괜\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1925 size=260 all=147367 active=14961 piece=ᅳᅳ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1804 size=280 all=150217 active=17811 piece=▁역시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1686 size=300 all=151647 active=19241 piece=▁당\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1686 min_freq=58\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1571 size=320 all=153686 active=9565 piece=▁그런\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1447 size=340 all=155950 active=11828 piece=▁슬\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1398 size=360 all=157849 active=13727 piece=하나\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1331 size=380 all=160086 active=15964 piece=▁아깝다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1263 size=400 all=161856 active=17734 piece=▁속\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1261 min_freq=53\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1222 size=420 all=163991 active=10150 piece=▁영화다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1175 size=440 all=165906 active=12065 piece=▁20\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1111 size=460 all=167765 active=13924 piece=지마\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1078 size=480 all=169289 active=15448 piece=▁돌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1027 size=500 all=171375 active=17534 piece=근데\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor: (149995, 117), Test Tensor: (49997, 113)\n",
      "Word Index Size: 12000, Index Word Size: 12000\n",
      "Train Labels: [0 1 0 0 1], Test Labels: [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# SentencePiece 학습 함수\n",
    "def train_sentencepiece(corpus, vocab_size=12000, model_prefix='korean_spm', model_type='bpe', output_file='filtered_corpus.txt'):\n",
    "\n",
    "    # 텍스트 파일로 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in corpus:\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "    # SentencePiece 모델 학습\n",
    "    spm.SentencePieceTrainer.train(input=output_file, model_prefix=model_prefix, vocab_size=vocab_size, model_type=model_type)\n",
    "    print(f\"모델 파일: {model_prefix}.model, {model_prefix}.vocab\")\n",
    "\n",
    "# SentencePiece Tokenizer 함수\n",
    "def sp_tokenize(model_path, corpus):\n",
    "\n",
    "    # 학습된 SentencePiece 모델 로드\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(f'{model_path}.model')\n",
    "\n",
    "    # 문장들을 토큰화하여 ID 시퀀스로 변환\n",
    "    tensor = [sp.EncodeAsIds(sentence) for sentence in corpus]\n",
    "    \n",
    "    # vocab 파일에서 단어 인덱스를 로드하여 word_index와 index_word 사전 생성\n",
    "    with open(f'{model_path}.vocab', 'r', encoding='utf-8') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {line.split('\\t')[0]: idx for idx, line in enumerate(vocab)}\n",
    "    index_word = {idx: line.split('\\t')[0] for idx, line in enumerate(vocab)}\n",
    "\n",
    "    # 패딩 적용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "\n",
    "    return tensor, word_index, index_word\n",
    "\n",
    "# 전체 작업을 실행하는 함수\n",
    "def process_and_tokenize(train_data, test_data, vocab_size=12000, model_prefix='korean_spm', model_type='bpe'):\n",
    "\n",
    "    # 리뷰 문장과 레이블 추출\n",
    "    train_corpus = train_data['document'].tolist()\n",
    "    train_labels = train_data['label'].values\n",
    "\n",
    "    test_corpus = test_data['document'].tolist()\n",
    "    test_labels = test_data['label'].values\n",
    "\n",
    "    # SentencePiece 모델 학습\n",
    "    train_sentencepiece(train_corpus, vocab_size, model_prefix, model_type)\n",
    "\n",
    "    # SentencePiece를 사용한 토큰화\n",
    "    train_tensor, word_index, index_word = sp_tokenize(model_prefix, train_corpus)\n",
    "    test_tensor, _, _ = sp_tokenize(model_prefix, test_corpus)\n",
    "\n",
    "    return train_tensor, test_tensor, word_index, index_word, train_labels, test_labels\n",
    "\n",
    "\n",
    "# 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 네이버 영화리뷰 데이터를 불러옵니다.\n",
    "    train_data = pd.read_csv('ratings_train.txt', sep='\\t')\n",
    "    test_data = pd.read_csv('ratings_test.txt', sep='\\t')\n",
    "\n",
    "    # 결측값 제거\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    # SentencePiece 모델 학습 및 토큰화 과정 실행\n",
    "    train_tensor, test_tensor, word_index, index_word, train_labels, test_labels = process_and_tokenize(train_data, test_data)\n",
    "\n",
    "    # 출력 확인 (학습된 데이터 텐서, 단어 사전, 레이블)\n",
    "    print(f\"Train Tensor: {train_tensor.shape}, Test Tensor: {test_tensor.shape}\")\n",
    "    print(f\"Word Index Size: {len(word_index)}, Index Word Size: {len(index_word)}\")\n",
    "    print(f\"Train Labels: {train_labels[:5]}, Test Labels: {test_labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef45117",
   "metadata": {},
   "source": [
    "### 모델 설계 후 학습 및 평가(함수화/모듈화)\n",
    "- 1차) LSTM 레이어층 그대로 사용\n",
    "- 2차) bidirectional, dropout, batchnomalization, 더 많은 Dense layer 추가\n",
    "\n",
    "#### Bidirectional 이란?\n",
    "LSTM은 시퀀스 데이터(한방향)를 읽음. 리뷰 텍스트같은 유형은 양방향으로 문맥을 파악하는 것이 중요.\n",
    "앞에서 뒤로, 뒤에서 앞으로 데이터를 처리 하는 역할을 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2571cd08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         1536000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 64)          82176     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,885,825\n",
      "Trainable params: 1,885,697\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D, Bidirectional, Dropout, BatchNormalization\n",
    "\n",
    "vocab_size = 12000  # SentencePiece에서 설정한 vocab_size와 동일\n",
    "\n",
    "# 감성 분석을 위한 LSTM 모델\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # 양방향 LSTM 추가\n",
    "model.add(Dropout(0.3))  # Dropout 추가\n",
    "model.add(LSTM(64, return_sequences=True))  # 추가 LSTM 레이어\n",
    "model.add(GlobalMaxPooling1D())  # Max Pooling\n",
    "model.add(BatchNormalization())  # Batch Normalization 추가\n",
    "model.add(Dense(64, activation='relu'))  # Dense 레이어 추가\n",
    "model.add(Dense(1, activation='sigmoid'))  # 출력층\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04d40cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 47s 20ms/step - loss: 0.1074 - accuracy: 0.9580 - val_loss: 0.5033 - val_accuracy: 0.8522\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0819 - accuracy: 0.9689 - val_loss: 0.5681 - val_accuracy: 0.8481\n",
      "Epoch 3/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0611 - accuracy: 0.9769 - val_loss: 0.6696 - val_accuracy: 0.8482\n",
      "Epoch 4/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0497 - accuracy: 0.9814 - val_loss: 0.7072 - val_accuracy: 0.8380\n",
      "Epoch 5/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0420 - accuracy: 0.9846 - val_loss: 0.7774 - val_accuracy: 0.8419\n",
      "Epoch 6/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0369 - accuracy: 0.9866 - val_loss: 0.8167 - val_accuracy: 0.8418\n",
      "Epoch 7/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0322 - accuracy: 0.9881 - val_loss: 0.8249 - val_accuracy: 0.8412\n",
      "Epoch 8/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0275 - accuracy: 0.9900 - val_loss: 0.9434 - val_accuracy: 0.8390\n",
      "Epoch 9/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0270 - accuracy: 0.9900 - val_loss: 0.8846 - val_accuracy: 0.8384\n",
      "Epoch 10/10\n",
      "2344/2344 [==============================] - 46s 20ms/step - loss: 0.0246 - accuracy: 0.9909 - val_loss: 0.9646 - val_accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_tensor, train_labels, epochs=10, batch_size=64, validation_data=(test_tensor, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9ba5c81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7490 - accuracy: 0.8266\n",
      "Test Accuracy: 0.8266295790672302\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(test_tensor, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fad7d5",
   "metadata": {},
   "source": [
    "### 실험 결과\n",
    "vocab size도 바꿔봤고, model type도 바꿔봤고, layer 층도 추가해봤다.\n",
    "하지만 성능이 올라가는 변화는 없었고 미미했다.\n",
    "\n",
    "> 가장 좋을 것 같은 파라미터로 설정\n",
    "- vocab size는 defalt 값 그대로 8000 고정.(많거나 적어도 과소/과대적합 문제)\n",
    "- model type은 'unigram' 설정.(bpe는 데이터가 많으면 성능이 떨어짐)\n",
    "- layer 층은 추가한 그대로 진행.(여러개 쌓을 수록 좋다. dropout은 한번)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8850c6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 파일: korean_spm.model, korean_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece 학습 함수\n",
    "def train_sentencepiece(corpus, vocab_size=8000, model_prefix='korean_spm', model_type='unigram', output_file='filtered_corpus.txt'):\n",
    "\n",
    "    # 텍스트 파일로 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in corpus:\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "    # SentencePiece 모델 학습\n",
    "    spm.SentencePieceTrainer.train(input=output_file, model_prefix=model_prefix, vocab_size=vocab_size, model_type=model_type)\n",
    "    print(f\"모델 파일: {model_prefix}.model, {model_prefix}.vocab\")\n",
    "\n",
    "# SentencePiece Tokenizer 함수\n",
    "def sp_tokenize(model_path, corpus):\n",
    "\n",
    "    # 학습된 SentencePiece 모델 로드\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(f'{model_path}.model')\n",
    "\n",
    "    # 문장들을 토큰화하여 ID 시퀀스로 변환\n",
    "    tensor = [sp.EncodeAsIds(sentence) for sentence in corpus]\n",
    "    \n",
    "    # vocab 파일에서 단어 인덱스를 로드하여 word_index와 index_word 사전 생성\n",
    "    with open(f'{model_path}.vocab', 'r', encoding='utf-8') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {line.split('\\t')[0]: idx for idx, line in enumerate(vocab)}\n",
    "    index_word = {idx: line.split('\\t')[0] for idx, line in enumerate(vocab)}\n",
    "\n",
    "    # 패딩 적용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "\n",
    "    return tensor, word_index, index_word\n",
    "\n",
    "# 전체 작업을 실행하는 함수\n",
    "def process_and_tokenize(train_data, test_data, vocab_size=8000, model_prefix='korean_spm', model_type='bpe'):\n",
    "\n",
    "    # 리뷰 문장과 레이블 추출\n",
    "    train_corpus = train_data['document'].tolist()\n",
    "    train_labels = train_data['label'].values\n",
    "\n",
    "    test_corpus = test_data['document'].tolist()\n",
    "    test_labels = test_data['label'].values\n",
    "\n",
    "    # SentencePiece 모델 학습\n",
    "    train_sentencepiece(train_corpus, vocab_size, model_prefix, model_type)\n",
    "\n",
    "    # SentencePiece를 사용한 토큰화\n",
    "    train_tensor, word_index, index_word = sp_tokenize(model_prefix, train_corpus)\n",
    "    test_tensor, _, _ = sp_tokenize(model_prefix, test_corpus)\n",
    "\n",
    "    return train_tensor, test_tensor, word_index, index_word, train_labels, test_labels\n",
    "\n",
    "\n",
    "# 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 네이버 영화리뷰 데이터를 불러옵니다.\n",
    "    train_data = pd.read_csv('ratings_train.txt', sep='\\t')\n",
    "    test_data = pd.read_csv('ratings_test.txt', sep='\\t')\n",
    "\n",
    "    # 결측값 제거\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data = test_data.dropna(how='any')\n",
    "\n",
    "    # SentencePiece 모델 학습 및 토큰화 과정 실행\n",
    "    train_tensor, test_tensor, word_index, index_word, train_labels, test_labels = process_and_tokenize(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12e889e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 128)         1024000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, None, 64)          82176     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,373,825\n",
      "Trainable params: 1,373,697\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설계\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D, Bidirectional, Dropout, BatchNormalization\n",
    "\n",
    "vocab_size = 8000  # SentencePiece에서 설정한 vocab_size와 동일\n",
    "\n",
    "# 감성 분석을 위한 LSTM 모델\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # 양방향 LSTM 추가\n",
    "model.add(Dropout(0.3))  # Dropout 추가\n",
    "model.add(LSTM(64, return_sequences=True))  # 추가 LSTM 레이어\n",
    "model.add(GlobalMaxPooling1D())  # Max Pooling\n",
    "model.add(BatchNormalization())  # Batch Normalization 추가\n",
    "model.add(Dense(64, activation='relu'))  # Dense 레이어 추가\n",
    "model.add(Dense(1, activation='sigmoid'))  # 출력층\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "296e0d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2344/2344 [==============================] - 50s 21ms/step - loss: 0.0636 - accuracy: 0.9765 - val_loss: 0.7019 - val_accuracy: 0.8448\n",
      "Epoch 2/5\n",
      "2344/2344 [==============================] - 49s 21ms/step - loss: 0.0504 - accuracy: 0.9818 - val_loss: 0.7405 - val_accuracy: 0.8426\n",
      "Epoch 3/5\n",
      "2344/2344 [==============================] - 49s 21ms/step - loss: 0.0458 - accuracy: 0.9832 - val_loss: 0.7494 - val_accuracy: 0.8416\n",
      "Epoch 4/5\n",
      "2344/2344 [==============================] - 49s 21ms/step - loss: 0.0397 - accuracy: 0.9855 - val_loss: 0.8036 - val_accuracy: 0.8409\n",
      "Epoch 5/5\n",
      "2344/2344 [==============================] - 49s 21ms/step - loss: 0.0363 - accuracy: 0.9866 - val_loss: 0.8576 - val_accuracy: 0.8427\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_tensor, train_labels, epochs=5, batch_size=64, validation_data=(test_tensor, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e4d1b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.842499349336323\n",
      "Precision: 0.8491807248365486\n",
      "Recall: 0.8359222915259624\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 (f1 score, precision, recall)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# 예측값 얻기 (이진 분류이므로 0.5 기준으로 예측)\n",
    "predictions = (model.predict(test_tensor) > 0.5).astype(\"int32\")\n",
    "\n",
    "# F1 Score 계산\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# Precision과 Recall 계산\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b901f",
   "metadata": {},
   "source": [
    "### koNLPy 형태소 분석기와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f80c2436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1078922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 한국어 형태소 분석기 Mecab 사용\n",
    "tokenizer = Mecab()\n",
    "\n",
    "# 불용어 리스트\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000, maxlen=30):\n",
    "    # 1. NaN 결측치 제거\n",
    "    train_data = train_data.dropna(how='any').copy()  # copy()로 복사본 생성\n",
    "    test_data = test_data.dropna(how='any').copy()\n",
    "\n",
    "    # 2. 중복 제거\n",
    "    train_data = train_data.drop_duplicates(subset=['document']).copy()\n",
    "    test_data = test_data.drop_duplicates(subset=['document']).copy()\n",
    "\n",
    "    # 3. 한국어 토크나이저로 토큰화 및 불용어 제거\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        tokenized_sentence = tokenizer.morphs(sentence)  # 토큰화\n",
    "        tokenized_sentence = [word for word in tokenized_sentence if word not in stopwords]  # 불용어 제거\n",
    "        X_train.append(tokenized_sentence)\n",
    "    \n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        tokenized_sentence = tokenizer.morphs(sentence)  # 토큰화\n",
    "        tokenized_sentence = [word for word in tokenized_sentence if word not in stopwords]  # 불용어 제거\n",
    "        X_test.append(tokenized_sentence)\n",
    "\n",
    "    # 4. 사전 word_to_index 구성\n",
    "    all_tokens = [word for sentence in X_train for word in sentence]\n",
    "    vocab = Counter(all_tokens)\n",
    "    vocab = vocab.most_common(num_words - 4)  # 상위 num_words-4개의 단어를 선택\n",
    "\n",
    "    word_to_index = {word: idx + 4 for idx, (word, _) in enumerate(vocab)}\n",
    "    word_to_index[\"<PAD>\"] = 0\n",
    "    word_to_index[\"<BOS>\"] = 1\n",
    "    word_to_index[\"<UNK>\"] = 2\n",
    "    word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "    # 5. 텍스트 데이터를 사전 인덱스로 변환\n",
    "    def text_to_index(sentences, word_to_index):\n",
    "        results = []\n",
    "        for sentence in sentences:\n",
    "            temp_result = []\n",
    "            for word in sentence:\n",
    "                if word in word_to_index:\n",
    "                    temp_result.append(word_to_index[word])\n",
    "                else:\n",
    "                    temp_result.append(word_to_index[\"<UNK>\"])\n",
    "            results.append(temp_result)\n",
    "        return results\n",
    "\n",
    "    X_train = text_to_index(X_train, word_to_index)\n",
    "    X_test = text_to_index(X_test, word_to_index)\n",
    "\n",
    "    # 6. 패딩 추가: 문장의 길이를 고정된 길이로 맞추기\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen, padding='pre')\n",
    "    X_test = pad_sequences(X_test, maxlen=maxlen, padding='pre')\n",
    "\n",
    "    # 7. 라벨 y_train, y_test 추출 (라벨 데이터)\n",
    "    y_train = np.array(train_data['label'])\n",
    "    y_test = np.array(test_data['label'])\n",
    "\n",
    "    return np.array(X_train), y_train, np.array(X_test), y_test, word_to_index\n",
    "\n",
    "# 데이터 전처리 수행\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f746b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2344/2344 [==============================] - 42s 17ms/step - loss: 0.4409 - accuracy: 0.7840 - val_loss: 0.9739 - val_accuracy: 0.5132\n",
      "Epoch 2/5\n",
      "2344/2344 [==============================] - 39s 17ms/step - loss: 0.3153 - accuracy: 0.8608 - val_loss: 1.0502 - val_accuracy: 0.5168\n",
      "Epoch 3/5\n",
      "2344/2344 [==============================] - 40s 17ms/step - loss: 0.2824 - accuracy: 0.8763 - val_loss: 1.0185 - val_accuracy: 0.5171\n",
      "Epoch 4/5\n",
      "2344/2344 [==============================] - 39s 17ms/step - loss: 0.2496 - accuracy: 0.8913 - val_loss: 1.1242 - val_accuracy: 0.5227\n",
      "Epoch 5/5\n",
      "2344/2344 [==============================] - 39s 17ms/step - loss: 0.2108 - accuracy: 0.9085 - val_loss: 1.3234 - val_accuracy: 0.5236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7b0c92ef1820>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def okt_tokenize(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = [word for word, _ in okt.pos(sentence)]\n",
    "        tokenized_corpus.append(tokens)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "    tensor.fit_on_texts(tokenized_corpus)\n",
    "    tensor = tensor.texts_to_sequences(tokenized_corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# KoNLPy를 이용한 토큰화\n",
    "train_tensor_okt = okt_tokenize(train_corpus)\n",
    "test_tensor_okt = okt_tokenize(test_corpus)\n",
    "\n",
    "# 동일한 모델을 이용하여 학습\n",
    "model.fit(train_tensor_okt, train_labels, epochs=5, batch_size=64, validation_data=(test_tensor_okt, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b06ea",
   "metadata": {},
   "source": [
    "#### KoNLPy 형태소 분석기와 비슷하니. accuracy는 잘 나왔어도 val accuracy는 SentencePiece 모델이 더 잘 나왔다.\n",
    "- SentencePiece: 0.85, KoNLPy: 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba6d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1512b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704c784d",
   "metadata": {},
   "source": [
    "## 회고\n",
    "오늘 NLP GoingDeeper 프로젝트를 진행해봤다. Google의 SentencePiece 모델을 구현해보고 적용해봤는데 생각보다 재밌었다. 전에도 네이버 영화리뷰 데이터로 Word2vec 을 사용해서 성능 개선해보고 그랬던 것 같은데, 그것과는 결이 같으면서 다른 모델을 사용해 성능을 분석하였고, 무엇보다 데이터의 특성에 따라 모델 파라미터 적용 여부가 극명하게 나눠질 수도 있다는게 흥미로웠다. 예를들어 vocab size를 늘린다거나 드랍아웃을 추가한다거나, 그 조정이 별거 아닐 수 있어도 영향을 많이 끼칠 수도, 적게 끼질 수도 있으니 좋은 모델이라고 다 좋은 것은 아니며 실험을 해보며 기록하고 측정하는 것이 중요하다고 느꼈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4e9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2ef08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba06fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e789797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee7d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de713df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
